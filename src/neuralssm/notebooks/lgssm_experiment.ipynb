{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(r'/Users/kostastsampourakis/Desktop/code/Python/projects/neuralssm/src/neuralssm')\n",
    "import jax # type: ignore\n",
    "from jax import numpy as jnp # type: ignore\n",
    "from jax import random as jr # type: ignore\n",
    "from jax import vmap # type: ignore\n",
    "import blackjax # type: ignore\n",
    "import jax.scipy.stats as jss # type: ignore\n",
    "from jax.scipy.special import logsumexp as lse # type: ignore\n",
    "from dynamax.utils.bijectors import RealToPSDBijector # type: ignore\n",
    "import numpy as onp # type: ignore\n",
    "\n",
    "from util.train import reshape_emissions\n",
    "from density_models import MAF\n",
    "from flax import nnx # type: ignore\n",
    "from matplotlib import pyplot as plt # type: ignore\n",
    "from util.param import params_from_tree, sample_prior, initialize, to_train_array, log_prior, get_unravel_fn, join_trees, tree_from_params\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd # type: ignore\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb # type: ignore\n",
    "from util.sample import sample_prior\n",
    "from simulators.ssm import LGSSM\n",
    "\n",
    "from datetime import date\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import scienceplots # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import matplotlib_inline # type: ignore\n",
    "plt.style.use(['science', 'ieee'])\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference loop\n",
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    @jax.jit\n",
    "    def one_step(state, rng_key):\n",
    "        state, _ = kernel(rng_key, state)\n",
    "        return state, state\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, states = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model and simulate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 100 # Number of emissions timesteps\n",
    "num_mcmc_steps = 1000 # Number of MCMC steps\n",
    "\n",
    "state_dim = 1\n",
    "emission_dim = 1\n",
    "input_dim = 0\n",
    "\n",
    "initial_mean = jnp.zeros(state_dim)\n",
    "initial_covariance = jnp.eye(state_dim) * 0.1\n",
    "\n",
    "dynamics_weights  = 0.9 * jnp.eye(state_dim)\n",
    "dynamics_bias = jnp.zeros(state_dim)\n",
    "dynamics_input_weights = jnp.zeros((state_dim, input_dim))\n",
    "dynamics_covariance = jnp.eye(state_dim) * 0.1\n",
    "\n",
    "emission_weights = jnp.eye(emission_dim, state_dim)\n",
    "emission_bias = jnp.zeros(emission_dim)\n",
    "emission_input_weights = jnp.zeros((emission_dim, input_dim))\n",
    "emission_covariance = jnp.eye(emission_dim) * 0.1\n",
    "\n",
    "# Initialize params and props_prior_prior_prior\n",
    "m = state_dim * (state_dim + 1) // 2\n",
    "dynamics_covariance_dist = tfd.MultivariateNormalDiag(loc=jnp.zeros(m), scale_diag= 0.1 * jnp.ones(m))\n",
    "emission_bias_dist = tfd.MultivariateNormalDiag(loc=jnp.zeros(emission_dim), scale_diag= 1.0 * jnp.ones(emission_dim))\n",
    "\n",
    "\n",
    "param_names = [['mean', 'cov'],\n",
    "               ['weights', 'bias', 'input_weights', 'cov'],\n",
    "               ['weights', 'bias', 'input_weights', 'cov']]\n",
    "\n",
    "prior_tree = [[initial_mean, initial_covariance],\n",
    "                [dynamics_weights, dynamics_bias, dynamics_input_weights, dynamics_covariance_dist],\n",
    "                [emission_weights, emission_bias_dist, emission_input_weights, emission_covariance]]\n",
    "\n",
    "is_constrained_tree = [[True, True], # The tree state at initialization\n",
    "                       [True, True, True, False], \n",
    "                       [True, True, True, True]]\n",
    "\n",
    "constrainers_tree  = [[None, RealToPSDBijector],\n",
    "                [None, None, None, RealToPSDBijector],\n",
    "                [None, None, None, RealToPSDBijector]] \n",
    "\n",
    "props_prior = initialize(prior_tree, param_names, constrainers_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample ***true*** params and emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = int(time.time() * 1000)  # Milliseconds for more granularity\n",
    "key = jax.random.PRNGKey(seed)\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "true_lgssm = LGSSM(state_dim, emission_dim)\n",
    "true_params = sample_prior(key, props_prior, 1)[0]\n",
    "true_params.from_unconstrained(props_prior)\n",
    "_, obs = true_lgssm.simulate(subkey, true_params, num_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TAF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Round 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample_and_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Sample SSM emissions and train TAF\u001b[39;00m\n\u001b[1;32m     35\u001b[0m key, subkey1, subkey2 \u001b[38;5;241m=\u001b[39m jr\u001b[38;5;241m.\u001b[39msplit(key, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m model, dataset \u001b[38;5;241m=\u001b[39m \u001b[43msample_and_train\u001b[49m(\n\u001b[1;32m     37\u001b[0m     key \u001b[38;5;241m=\u001b[39m subkey1,\n\u001b[1;32m     38\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     39\u001b[0m     ssmodel \u001b[38;5;241m=\u001b[39m test_lgssm,\n\u001b[1;32m     40\u001b[0m     lag \u001b[38;5;241m=\u001b[39m lag,\n\u001b[1;32m     41\u001b[0m     num_timesteps \u001b[38;5;241m=\u001b[39m num_timesteps, \n\u001b[1;32m     42\u001b[0m     props_prior \u001b[38;5;241m=\u001b[39m props_prior,\n\u001b[1;32m     43\u001b[0m     params_sample \u001b[38;5;241m=\u001b[39m params_sample,\n\u001b[1;32m     44\u001b[0m     prev_dataset \u001b[38;5;241m=\u001b[39m dataset, \n\u001b[1;32m     45\u001b[0m     num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     46\u001b[0m     learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m     47\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Sample new parameters using TAF likelihood and MCMC\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-Sampling new parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_and_train' is not defined"
     ]
    }
   ],
   "source": [
    "lag = 10\n",
    "din = emission_dim\n",
    "n_params = to_train_array(true_params, props_prior).shape[0]\n",
    "dcond = lag * emission_dim + n_params\n",
    "reverse = True\n",
    "random_order = False\n",
    "batch_norm = False\n",
    "dropout = False\n",
    "nmades = 5\n",
    "dhidden = 32\n",
    "nhidden = 5\n",
    "\n",
    "# Initialize models\n",
    "model = MAF(din, nmades, dhidden, nhidden,'relu', dcond, nnx.Rngs(0), random_order, reverse, batch_norm, dropout)\n",
    "test_lgssm = LGSSM(state_dim, emission_dim)\n",
    "\n",
    "# Sample parameters for first round\n",
    "num_samples = 10\n",
    "key, subkey = jr.split(key)\n",
    "params_sample = sample_prior(subkey, props_prior, num_samples) # Here, output params are in mixed constrained/unconstrained form\n",
    "                                                              # The trainable params (given in prior by dist) are unconstrained\n",
    "                                                              # whereas the not-trainable params (given in prior by arrays) are constrained\n",
    "                                                              # In the trainer, the cond_params are appended to the dataset and then the params are converted\n",
    "                                                              # to constrained form before being passed to the model for simulation. \n",
    "dataset = jnp.array([])\n",
    "num_rounds = 2\n",
    "params_samples_allrounds = []\n",
    "for r in range(num_rounds):\n",
    "    print(f\"-Round {r}\")\n",
    "\n",
    "    # Add previous samples to samples\n",
    "    params_samples_allrounds.append(params_sample)    \n",
    "\n",
    "    # Sample SSM emissions and train TAF\n",
    "    key, subkey1, subkey2 = jr.split(key, 3)\n",
    "    model, dataset = sample_and_train(\n",
    "        key = subkey1,\n",
    "        model = model,\n",
    "        ssmodel = test_lgssm,\n",
    "        lag = lag,\n",
    "        num_timesteps = num_timesteps, \n",
    "        props_prior = props_prior,\n",
    "        params_sample = params_sample,\n",
    "        prev_dataset = dataset, \n",
    "        num_epochs = 20,\n",
    "        learning_rate = 1 * 1e-4,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Sample new parameters using TAF likelihood and MCMC\n",
    "    print(\"-Sampling new parameters\")\n",
    "\n",
    "    ## Define TAF-based logdensity function\n",
    "    def taf_logdensity_fn(cond_params):\n",
    "        lagged_emissions = reshape_emissions(emissions, lag)\n",
    "        tile_cond_params = jnp.tile(cond_params, (lagged_emissions.shape[0], 1))\n",
    "        lp = -model.loss_fn(jnp.concatenate([tile_cond_params, lagged_emissions], axis=1))\n",
    "        lp += log_prior(cond_params, props_prior)\n",
    "        return lp\n",
    "\n",
    "    ## Initialize MCMC chain and kernel\n",
    "    rng_key = jax.random.key(int(date.today().strftime(\"%Y%m%d\")))\n",
    "    initial_cond_params = to_train_array(sample_prior(rng_key, props_prior, 1)[0], props_prior)\n",
    "    taf_random_walk = blackjax.additive_step_random_walk(taf_logdensity_fn, blackjax.mcmc.random_walk.normal(0.1))\n",
    "    taf_initial_state = taf_random_walk.init(initial_cond_params)\n",
    "    taf_kernel = jax.jit(taf_random_walk.step)\n",
    "\n",
    "    ## Run inference loop\n",
    "    rng_key, sample_key1, sample_key2 = jax.random.split(rng_key, 3)\n",
    "    taf_mcmc_states = inference_loop(sample_key1, taf_kernel, taf_initial_state, num_mcmc_steps)\n",
    "    positions = taf_mcmc_states.position[-num_samples:]\n",
    "    params_sample = []\n",
    "    print(\"-Adding new params\")\n",
    "    for cond_param in positions:\n",
    "        unravel_fn = get_unravel_fn(true_params, props_prior)\n",
    "        unravel = unravel_fn(cond_param)\n",
    "        tree = tree_from_params(true_params)\n",
    "        new_tree = join_trees(unravel, tree, props_prior)\n",
    "        param = params_from_tree(new_tree, param_names, is_constrained_tree)\n",
    "        params_sample.append(param)\n",
    "\n",
    "# Add samples from final round\n",
    "params_samples_allrounds.append(params_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood plots (for MAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Destination /Users/kostastsampourakis/Desktop/code/Python/projects/neuralssm/src/neuralssm/checkpoints already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/kostastsampourakis/Desktop/code/Python/projects/neuralssm/src/neuralssm/checkpoints/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Save the parameters\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mcheckpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/jaxenv/lib/python3.11/site-packages/orbax/checkpoint/checkpointer.py:201\u001b[0m, in \u001b[0;36mCheckpointer.save\u001b[0;34m(self, directory, force, *args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m       directory\u001b[38;5;241m.\u001b[39mrmtree()  \u001b[38;5;66;03m# Post-sync handled by create_tmp_directory.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    202\u001b[0m ckpt_args \u001b[38;5;241m=\u001b[39m construct_checkpoint_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    203\u001b[0m tmpdir \u001b[38;5;241m=\u001b[39m asyncio_utils\u001b[38;5;241m.\u001b[39mrun_sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_temporary_path(directory))\n",
      "\u001b[0;31mValueError\u001b[0m: Destination /Users/kostastsampourakis/Desktop/code/Python/projects/neuralssm/src/neuralssm/checkpoints already exists."
     ]
    }
   ],
   "source": [
    "from flax import nnx\n",
    "import orbax.checkpoint as ocp\n",
    "import jax.numpy as jnp\n",
    "\n",
    "graphdef, state = nnx.split(model)\n",
    "\n",
    "# Create a checkpointer\n",
    "checkpointer = ocp.PyTreeCheckpointer()\n",
    "save_dir = \"/Users/kostastsampourakis/Desktop/code/Python/projects/neuralssm/src/neuralssm/checkpoints/\"\n",
    "\n",
    "# Save the parameters\n",
    "checkpointer.save(save_dir, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Restore the checkpoint back to its `nnx.State` structure - need an abstract reference.\n",
    "# abstract_model = nnx.eval_shape(graphdef)\n",
    "# graphdef, abstract_state = nnx.split(abstract_model)\n",
    "# print('The abstract NNX state (all leaves are abstract arrays):')\n",
    "# nnx.display(abstract_state)\n",
    "abstract_model = MAF(din, nmades, dhidden, nhidden,'relu', dcond, nnx.Rngs(0), random_order, reverse, batch_norm, dropout)\n",
    "graphdef, abstract_state = nnx.split(abstract_model)\n",
    "\n",
    "state_restored = checkpointer.restore(save_dir, abstract_state)\n",
    "model = nnx.merge(graphdef, state_restored)\n",
    "# jax.tree.map(np.testing.assert_array_equal, state, state_restored)\n",
    "# print('NNX State restored: ')\n",
    "# nnx.display(state_restored)\n",
    "\n",
    "# The model is now good to use!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covs = []\n",
    "# for param in params_samples_allrounds[0]:\n",
    "#     param.to_unconstrained(props)\n",
    "#     covs.append(param.dynamics.cov.value)\n",
    "# thetas = jnp.array(covs).flatten()\n",
    "# thetas = jnp.sort(thetas)\n",
    "\n",
    "# log_density_values = vmap(lambda param: logdensity_fn(model, prior, param, emissions, lag))(thetas[:, None])\n",
    "# log_likelihood_values = vmap(lambda param: loglik(model, param, emissions, lag))(thetas[:, None])\n",
    "# true_log_likelihood_values = vmap(lambda theta: true_loglik(theta, emissions, state_dim, emission_dim, props, example_params, param_names, is_constrained_tree))(thetas[:, None])\n",
    "\n",
    "# true_log_likelihood_values = true_log_likelihood_values - jnp.min(true_log_likelihood_values)\n",
    "# true_log_likelihood_values = true_log_likelihood_values / jnp.max(true_log_likelihood_values)\n",
    "# log_density_values = log_density_values - jnp.min(log_density_values)\n",
    "# log_density_values = log_density_values / jnp.max(log_density_values)\n",
    "# log_likelihood_values = log_likelihood_values - jnp.min(log_likelihood_values)\n",
    "# log_likelihood_values = log_likelihood_values / jnp.max(log_likelihood_values)\n",
    "\n",
    "# plt.plot(thetas, log_density_values, label='log density')\n",
    "# plt.plot(thetas, log_likelihood_values, label='loglik')\n",
    "# plt.plot(thetas, true_log_likelihood_values, label='True loglik')\n",
    "# plt.vlines(thetas, 0, 1, color='blue', alpha=0.02)\n",
    "# plt.vlines(RealToPSDBijector().inverse(true_params.dynamics.cov.value), 0, 1, color='red', label='True cov', linestyle='--', alpha=0.2)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# print('true loglik argmax', thetas[jnp.argmax(true_log_likelihood_values)])\n",
    "# print('est loglik argmax', thetas[jnp.argmax(log_likelihood_values)])\n",
    "# print('est log density argmax', thetas[jnp.argmax(log_density_values)])\n",
    "# print('true param value', RealToPSDBijector().inverse(true_params.dynamics.cov.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 1000 # Number of particles for BPF\n",
    "num_iters = 5 # Number of iterations for BPF likelihood estimator\n",
    "\n",
    "# Define the log-density function\n",
    "def bpf_logdensity_fn(cond_params):\n",
    "    unravel_fn = get_unravel_fn(example_params, props_prior)\n",
    "    unravel = unravel_fn(cond_params)\n",
    "    tree = tree_from_params(example_params)\n",
    "    new_tree = join_trees(unravel, tree, props_prior)\n",
    "    params = params_from_tree(new_tree, param_names, is_constrained_tree)\n",
    "    params.from_unconstrained(props_prior)\n",
    "    lps = []\n",
    "    for _ in range(num_iters):\n",
    "        _, lp = bpf(params, test_lgssm, emissions, num_particles, key)\n",
    "        lp += log_prior(cond_params, props_prior)\n",
    "        lps.append(lp)\n",
    "    return jnp.mean(jnp.array(lps))\n",
    "\n",
    "def taf_logdensity_fn(cond_params):\n",
    "    lagged_emissions = reshape_emissions(emissions, lag)\n",
    "    tile_cond_params = jnp.tile(cond_params, (lagged_emissions.shape[0], 1))\n",
    "    lp = -model.loss_fn(jnp.concatenate([tile_cond_params, lagged_emissions], axis=1))\n",
    "    lp += log_prior(cond_params, props_prior)\n",
    "    return lp\n",
    "\n",
    "# Initialize MCMC chain and kernel\n",
    "rng_key = jax.random.key(int(date.today().strftime(\"%Y%m%d\")))\n",
    "initial_cond_params = to_train_array(sample_ssm_params(rng_key, props_prior, 1)[0], props_prior)\n",
    "\n",
    "rw_sigma = 0.1\n",
    "taf_random_walk = blackjax.additive_step_random_walk(taf_logdensity_fn, blackjax.mcmc.random_walk.normal(rw_sigma))\n",
    "bpf_random_walk = blackjax.additive_step_random_walk(bpf_logdensity_fn, blackjax.mcmc.random_walk.normal(rw_sigma))\n",
    "\n",
    "taf_initial_state = taf_random_walk.init(initial_cond_params)\n",
    "taf_kernel = jax.jit(taf_random_walk.step)\n",
    "\n",
    "bpf_initial_state = bpf_random_walk.init(initial_cond_params)\n",
    "bpf_kernel = jax.jit(bpf_random_walk.step)\n",
    "\n",
    "# Run inference loop\n",
    "rng_key, sample_key1, sample_key2 = jax.random.split(rng_key, 3)\n",
    "taf_mcmc_states = inference_loop(sample_key1, taf_kernel, taf_initial_state, num_mcmc_steps)\n",
    "bpf_mcmc_states = inference_loop(sample_key2, bpf_kernel, bpf_initial_state, num_mcmc_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (MC estimate of posterior normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from output_utils import true_loglik\n",
    "\n",
    "# with open('baseline.csv', mode='r') as file:\n",
    "#     csvFile = csv.reader(file)\n",
    "#     for line_number, line in enumerate(csvFile):\n",
    "#         baseline_error = float(line[0])\n",
    "\n",
    "# if baseline_error == 0.0:\n",
    "seed = 121241378123 \n",
    "num_reps = 20\n",
    "baseline_num_samples = int(1e2)\n",
    "for i in range(num_reps):\n",
    "    print(f\"------------------ Repetition {i} ------------------\")\n",
    "    key, subkey = jr.split(key)\n",
    "    true_lgssm = LGSSM(state_dim, emission_dim)\n",
    "    [true_params, example_params] = sample_ssm_params(key, props_prior, 2)\n",
    "    true_params.from_unconstrained(props_prior)\n",
    "    _, emissions = true_lgssm.simulate(subkey, true_params, num_timesteps)\n",
    "    covs = []\n",
    "    key, subkey = jr.split(key)\n",
    "\n",
    "    prior_samples = sample_ssm_params(subkey, props_prior, baseline_num_samples)\n",
    "    for param in prior_samples:\n",
    "        param.to_unconstrained(props_prior)\n",
    "        covs.append(param.dynamics.cov.value)\n",
    "    thetas = jnp.array(covs)\n",
    "    ctrue_loglik = lambda cond_param: true_loglik(cond_param, emissions, state_dim, emission_dim, props_prior, example_params, param_names, is_constrained_tree)\n",
    "    true_loglik_vals = vmap(ctrue_loglik)(thetas)\n",
    "    logZ_hat = lse(true_loglik_vals) - jnp.log(baseline_num_samples)\n",
    "    baseline_log_posterior = lambda cond_param: ctrue_loglik(cond_param) + log_prior(cond_param, props_prior) - logZ_hat\n",
    "    true_params.to_unconstrained(props_prior)\n",
    "    baseline_error = -baseline_log_posterior(to_train_array(true_params, props_prior))\n",
    "    true_params.from_unconstrained(props_prior)\n",
    "    with open(\"baseline.csv\", mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([baseline_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel density estimation and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "taf_kernel_points = taf_mcmc_states.position.T\n",
    "posterior_mean = jnp.mean(taf_kernel_points, axis=1)\n",
    "taf_kde = jss.gaussian_kde(taf_kernel_points)\n",
    "taf_num_simulations = num_rounds * num_samples * num_timesteps\n",
    "taf_error = -jnp.log(taf_kde.evaluate(RealToPSDBijector().inverse(true_params.dynamics.cov.value)))\n",
    "\n",
    "bpf_kernel_points = bpf_mcmc_states.position.T\n",
    "bpf_kde = jss.gaussian_kde(bpf_kernel_points)\n",
    "bpf_num_simulations = num_particles * num_timesteps * num_mcmc_steps * num_iters\n",
    "bpf_error = -jnp.log(bpf_kde.evaluate(RealToPSDBijector().inverse(true_params.dynamics.cov.value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print outputs to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data to be written\n",
    "header_row = [\"BPF error\", \"BPF num_sims\", \"TAF error\", \"TAF num_sims\", \"num_timesteps\", \"num_mcmc_steps\", \"num_reps\", \"lag\", \"num_samples\", \"num_rounds\", \"num_particles\", \"num_iters\"]\n",
    "new_row = [bpf_error[0], jnp.log10(bpf_num_simulations), taf_error[0], jnp.log10(taf_num_simulations)]\n",
    "\n",
    "# Specify the file name\n",
    "file_name = \"output_lgssm.csv\"\n",
    "# Write the data to the CSV file\n",
    "with open(file_name, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(new_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "bpf_errors = []\n",
    "taf_errors = []\n",
    "bpf_num_sims = []\n",
    "taf_num_sims = []\n",
    "file_name = \"output_lgssm.csv\"\n",
    "with open(file_name, mode='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    for line_number, line in enumerate(csvFile):\n",
    "        if line_number == 0:\n",
    "            continue\n",
    "        else:\n",
    "            bpf_errors.append(float(line[0]))\n",
    "            bpf_num_sims.append(float(line[1]))\n",
    "            taf_errors.append(float(line[2]))\n",
    "            taf_num_sims.append(float(line[3]))\n",
    "    bpf_errors = jnp.array(bpf_errors)\n",
    "    taf_errors = jnp.array(taf_errors)\n",
    "    bpf_num_sims = jnp.array(bpf_num_sims)\n",
    "    taf_num_sims = jnp.array(taf_num_sims)\n",
    "\n",
    "    bpf_errors = bpf_errors[jnp.argsort(bpf_num_sims)]\n",
    "    bpf_num_sims = bpf_num_sims[jnp.argsort(bpf_num_sims)]\n",
    "    taf_errors = taf_errors[jnp.argsort(taf_num_sims)]\n",
    "    taf_num_sims = taf_num_sims[jnp.argsort(taf_num_sims)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far I am varying the numbers of simulations by varying:\n",
    "- `num_samples` and `num_rounds` for TAF\n",
    "- `num_particles` and `num_iters` for BPF\n",
    "\n",
    "The reason that BPF has large `num_sims` is that the number of MCMC steps is a factor of the number of simulations. If I take it out something like a per MCMC step count.\n",
    "\n",
    "I might need to do an averaging of the errors for the TAF model for each experimental setting. This is because I see there is some variability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_num_sims-3, bpf_errors, linewidth=0.5, color='gray', markerfacecolor='blue', markeredgecolor='blue', marker='s', markersize=2.0, linestyle='--', label='BPF')\n",
    "plt.plot(taf_num_sims, taf_errors, linewidth=0.5, color='gray', markerfacecolor='orange', markeredgecolor='orange', marker='^', markersize=2.0, linestyle='--', label='TAF')\n",
    "plt.hlines(baseline_error, jnp.array([0.0]), jnp.max(jnp.array([bpf_num_sims, taf_num_sims])), color='red', linewidth=0.4, label='Baseline')\n",
    "plt.xlabel('Number of simulations')\n",
    "# plt.xticks(jnp.linspace(0, 7, 8), [f'$10^{i}$' for i in range(8)])\n",
    "plt.ylabel('Error')\n",
    "plt.title('LGSSM - dynamics covariance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
